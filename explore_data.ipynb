{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "explore_data.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMnziKkrF9ifjDv6ru3h8kd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nathanbollig/distributed-mutation/blob/main/explore_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY1Lfh5kgtL7"
      },
      "source": [
        "# Explore Synthetic Data and Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKPEjUI1g0RJ"
      },
      "source": [
        "In Phase 1 and 2 of this project, synthetic viral sequence data is generated and a classification model is trained on 80% of this data. The result of running the `phases_1_and_2.py` script is a set results objects saved to disk into 6 pickle files. After performing Phase 1 and Phase 2, this notebook loads in these files and explores their contents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddXSXZAsxPkP"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO0lhqPxgnrO"
      },
      "source": [
        "## Phase 1 and 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZauKNjQ9gum7"
      },
      "source": [
        "This section follows the instructions in the project README to generate data and train a classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hne7sMQngizy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79d0e38c-3f8b-4c53-fa65-1128d0812e1e"
      },
      "source": [
        "# Clone the git repository\n",
        "!git clone https://github.com/nathanbollig/distributed-mutation"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'distributed-mutation'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (36/36), done.\u001b[K\n",
            "remote: Total 37 (delta 14), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (37/37), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FFupudVUg86t",
        "outputId": "053b4370-2a4f-4a08-d7ac-4bd7a5917b9d"
      },
      "source": [
        "# Apply requirements.txt\n",
        "%cd distributed-mutation\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/distributed-mutation\n",
            "Collecting hmmlearn==0.2.6\n",
            "  Downloading hmmlearn-0.2.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 5.5 MB/s \n",
            "\u001b[?25hCollecting Keras==2.2.4\n",
            "  Downloading Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
            "\u001b[K     |████████████████████████████████| 312 kB 35.0 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.22.2\n",
            "  Downloading scikit_learn-0.22.2-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 16.7 MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.15.5\n",
            "  Downloading tensorflow-1.15.5-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 1.4 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.7/dist-packages (from hmmlearn==0.2.6->-r requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from hmmlearn==0.2.6->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.4->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.4->-r requirements.txt (line 2)) (1.15.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.4->-r requirements.txt (line 2)) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.4->-r requirements.txt (line 2)) (3.13)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2->-r requirements.txt (line 3)) (1.1.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 56.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->-r requirements.txt (line 4)) (0.37.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->-r requirements.txt (line 4)) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->-r requirements.txt (line 4)) (1.13.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->-r requirements.txt (line 4)) (1.41.1)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 65.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->-r requirements.txt (line 4)) (0.12.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->-r requirements.txt (line 4)) (0.2.0)\n",
            "Collecting numpy>=1.10\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->-r requirements.txt (line 4)) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->-r requirements.txt (line 4)) (1.1.0)\n",
            "Collecting h5py\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 47.6 MB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->-r requirements.txt (line 4)) (3.17.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->-r requirements.txt (line 4)) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->-r requirements.txt (line 4)) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->-r requirements.txt (line 4)) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->-r requirements.txt (line 4)) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->-r requirements.txt (line 4)) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->-r requirements.txt (line 4)) (3.6.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=1342b21e7220d769c6c2da7715672ace17a5709bf4ca76d2dfff42ea52949614\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: numpy, h5py, tensorflow-estimator, tensorboard, scikit-learn, keras-applications, gast, tensorflow, Keras, hmmlearn\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "  Attempting uninstall: Keras\n",
            "    Found existing installation: keras 2.7.0\n",
            "    Uninstalling keras-2.7.0:\n",
            "      Successfully uninstalled keras-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.14.1 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.15.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Keras-2.2.4 gast-0.2.2 h5py-2.10.0 hmmlearn-0.2.6 keras-applications-1.0.8 numpy-1.18.5 scikit-learn-0.22.2 tensorboard-1.15.0 tensorflow-1.15.5 tensorflow-estimator-1.15.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtauIY2Xi6Mr"
      },
      "source": [
        "The above conflicts are acceptable because these conflicts are with pre-installed packages in this environment that I don't believe are needed for executing my code. May now need to restart the runtime (and therefore change directory again)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhfwCQ9BiifG",
        "outputId": "11b54a0c-93dd-4c91-929e-504c4318398b"
      },
      "source": [
        "# Run script\n",
        "%cd distributed-mutation\n",
        "!bash phases_1_and_2.sh"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'distributed-mutation'\n",
            "/content/distributed-mutation\n",
            "Using TensorFlow backend.\n",
            "tcmalloc: large alloc 4800004096 bytes == 0x556b1641a000 @  0x7f2121a89001 0x7f211f5ed7b5 0x7f211f651c00 0x7f211f653a9f 0x7f211f6ea078 0x556af3233544 0x556af3233240 0x556af32a7627 0x556af32a19ee 0x556af3234bda 0x556af32a2915 0x556af3234afa 0x556af32a2915 0x556af32a19ee 0x556af3234bda 0x556af32a3737 0x556af32a19ee 0x556af32a16f3 0x556af336b4c2 0x556af336b83d 0x556af336b6e6 0x556af3343163 0x556af3342e0c 0x7f2120871bf7 0x556af3342cea\n",
            "tcmalloc: large alloc 3840000000 bytes == 0x556c345be000 @  0x7f2121a871e7 0x7f211f5ed631 0x7f211f651cc8 0x7f211f651de3 0x7f211f6dcf06 0x7f211f6dd368 0x556af331b409 0x556af32a2e7a 0x556af32a19ee 0x556af3234bda 0x556af32a3737 0x556af32a19ee 0x556af3234bda 0x556af32a2915 0x556af3325cf8 0x556af331bcae 0x556af330bae5 0x556af3242224 0x556af32730a4 0x556af3233c52 0x556af32a6c25 0x556af32a1ced 0x556af3234bda 0x556af32a3737 0x556af32a19ee 0x556af3234bda 0x556af32a3737 0x556af32a19ee 0x556af32a16f3 0x556af336b4c2 0x556af336b83d\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "2021-11-11 22:12:04.388486: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2021-11-11 22:12:04.425294: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-11-11 22:12:04.427573: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x556af47b8f40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-11-11 22:12:04.427651: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "800000/800000 [==============================] - 17s 21us/step - loss: 0.0919 - acc: 0.9739\n",
            "Epoch 2/10\n",
            "800000/800000 [==============================] - 17s 21us/step - loss: 0.0565 - acc: 0.9789\n",
            "Epoch 3/10\n",
            "800000/800000 [==============================] - 17s 21us/step - loss: 0.0564 - acc: 0.9790\n",
            "Epoch 4/10\n",
            "800000/800000 [==============================] - 17s 21us/step - loss: 0.0564 - acc: 0.9790\n",
            "Epoch 5/10\n",
            "800000/800000 [==============================] - 17s 21us/step - loss: 0.0564 - acc: 0.9790\n",
            "Epoch 6/10\n",
            "800000/800000 [==============================] - 17s 21us/step - loss: 0.0564 - acc: 0.9790\n",
            "Epoch 7/10\n",
            "800000/800000 [==============================] - 17s 21us/step - loss: 0.0564 - acc: 0.9791\n",
            "Epoch 8/10\n",
            "800000/800000 [==============================] - 17s 21us/step - loss: 0.0564 - acc: 0.9790\n",
            "Epoch 9/10\n",
            "800000/800000 [==============================] - 17s 21us/step - loss: 0.0564 - acc: 0.9789\n",
            "Epoch 10/10\n",
            "800000/800000 [==============================] - 17s 21us/step - loss: 0.0564 - acc: 0.9790\n",
            "Train Accuracy: 97.95\n",
            "Validation Accuracy: 97.97\n",
            "tcmalloc: large alloc 3840008192 bytes == 0x556af9dbe000 @  0x7f2121a871e7 0x556af3264f98 0x556af322fe27 0x7f211f6e0f0d 0x556af334890c 0x556af32336f2 0x7f211f6e1bb8 0x556af334890c 0x556af32343ce 0x556af32342f9 0x556af32e4f94 0x556af32e48a8 0x556af32e3f3c 0x556af31d5992 0x556af323346c 0x556af3233240 0x556af32a7627 0x556af32a19ee 0x556af32a16f3 0x556af336b4c2 0x556af336b83d 0x556af336b6e6 0x556af3343163 0x556af3342e0c 0x7f2120871bf7 0x556af3342cea\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3KxablrkQMw"
      },
      "source": [
        "## Explore Phase 1 and 2 Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg26JS-TkcI3"
      },
      "source": [
        "### Read in results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmpRz7Z4k7Fl"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhXcQyLpkf1E",
        "outputId": "d6edd535-dd5f-4f7a-c7b3-394f25f55868"
      },
      "source": [
        "with open(\"model.pkl\", 'rb') as pfile:\n",
        "    model_tuple = pickle.load(pfile)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using TensorFlow backend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_daNhizw-1S5"
      },
      "source": [
        "with open(\"aa_vocab.pkl\", 'rb') as pfile:\n",
        "    aa_vocab = pickle.load(pfile)\n",
        "with open(\"generator.pkl\", 'rb') as pfile:\n",
        "    gen = pickle.load(pfile)\n",
        "with open(\"data_train.pkl\", 'rb') as pfile:\n",
        "    data_train = pickle.load(pfile)\n",
        "with open(\"data_val.pkl\", 'rb') as pfile:\n",
        "    data_val = pickle.load(pfile)\n",
        "with open(\"data_test.pkl\", 'rb') as pfile:\n",
        "    data_test = pickle.load(pfile)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7kQNZ9vlQm7"
      },
      "source": [
        "### Unpack items"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLsUepWDlAXP"
      },
      "source": [
        "model, result = model_tuple"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "to5jMXZylXcq"
      },
      "source": [
        "X_train, y_train = data_train\n",
        "X_val, y_val = data_val\n",
        "X_test, y_test = data_test"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifFVCGFClpRu"
      },
      "source": [
        "### Model and validation results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYlC3qNbluus"
      },
      "source": [
        "The Keras model object is stored in `model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVZIgrkcluEk",
        "outputId": "03783d2a-d77d-467e-8a88-8e8c12ee28af"
      },
      "source": [
        "type(model)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "keras.engine.sequential.Sequential"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sANnES3Rl6ki"
      },
      "source": [
        "During the training of this model, a dictionary was created of training set and validation set performance. We can display the values in this dictionary to see the accuracy of the stored model on its training and validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhYGeA76mBRb",
        "outputId": "aae45b83-77f3-4662-f8c8-14694be50469"
      },
      "source": [
        "result"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_train_accuracy': 0.979525, 'model_val_accuracy': 0.97968}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm3-FWr5mQwg"
      },
      "source": [
        "### Synthetic sequence data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbJubNbQmTxG"
      },
      "source": [
        "The data are sequences (variable names prefixed with `X`) and their labels (prefixed with `y`). Data is split into training, validation, and test sets. The training set was used to train the model and the validation set was used to measure the performance reported in the `result` dictionary. The test set has not yet been used for model training or evaluation.\n",
        "\n",
        "The data should be 80% training, 10% validation, and 10% test, with the total number of sequences as specified in the `phases_1_and_2.sh` script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFL6G855m_g8",
        "outputId": "b7d46fd4-b1f9-47a2-fdc4-e1a8e9aea7a5"
      },
      "source": [
        "len(X_train), len(X_val), len(X_test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800000, 100000, 100000)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrbJmM0YnJjn"
      },
      "source": [
        "As expected, there are the same number of labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13crO8qCnHj1",
        "outputId": "2adef25e-896b-4063-c4f2-1877b26845e8"
      },
      "source": [
        "len(y_train), len(y_val), len(y_test)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800000, 100000, 100000)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz3PLhOPnQk7"
      },
      "source": [
        "The sequence variables (`X_`*) are numpy arrays, where each sequence is represented by a 60 x 20 matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm53fyLVnSV3",
        "outputId": "90ad8307-0acc-4e12-819c-8f8cce65ed99"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800000, 60, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGrWHlrVnlOB"
      },
      "source": [
        "Each of the 60 positions in the sequence is represented by a one-hot vector of length 20. We assume that 20 is the size of the character alphabet. For example, a single sequence looks like the following matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-iN9FUNnmPA",
        "outputId": "a9ddce24-2db2-452c-f583-55b2668658de"
      },
      "source": [
        "X_train[42]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       ...,\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6qHBcU8omr5"
      },
      "source": [
        "The binary sequence label is an integer. The value 1 represents positive, and 0 represents negative. For example, the label for the above sequence is shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPnpjB-CoxOI",
        "outputId": "3cc6f9fc-48fd-4f4a-f5ca-6258c95f26b7"
      },
      "source": [
        "y_train[42]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O67nQig0oiBE"
      },
      "source": [
        "### Applying the model to a sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9nQXZ-ooz3H"
      },
      "source": [
        "We can apply the stored model to a sequence to get a prediction, using the TensorFlow model objects's API. For example, suppose we want to apply the model to sequences at index 42 and 43 in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMrF2REeo8NN",
        "outputId": "88d09f4c-886d-4c24-df18-15654126897e"
      },
      "source": [
        "model.predict(X_train[42:44])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00996366],\n",
              "       [0.01680388]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZumGgHzLpXjN"
      },
      "source": [
        "We see the prediction for each sequence as a number between 0 and 1. In this case, they are both close to 1, indicating higher confidence of positive. These predictions are correct in this context, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kik3JggMplnq",
        "outputId": "96cc6163-0e9a-466c-d185-783e842605ce"
      },
      "source": [
        "y_train[42:44]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VNE61vkppoV"
      },
      "source": [
        "### Amino acid vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ7V2OijpvPI"
      },
      "source": [
        "The pickle file also included the amino acid vocabulary used for the encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3gHAGRHp3V1",
        "outputId": "3f8489cd-b725-4c94-8894-904fcdcf2d1d"
      },
      "source": [
        "print(aa_vocab)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drf89Wh7p8tb",
        "outputId": "953f342d-e811-47ee-f749-383504e35851"
      },
      "source": [
        "len(aa_vocab)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIOgt0aSp61b"
      },
      "source": [
        "This associates an index in the range 0-19 (as described above in relation to the one-hot representation of sequences) to a specific character that reflects an amino acid in the biological sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "974voIvGqOzR"
      },
      "source": [
        "### Markov model (HMMGenerator object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbOH7Md5qTx_"
      },
      "source": [
        "Finally, the pickle file includes the `HMMGenerator` object used to synthesize the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4tN92tYqZMS",
        "outputId": "be82506e-2f90-44c1-fe2a-9d8f3213ecad"
      },
      "source": [
        "type(gen)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HMM_generator_motif.HMMGenerator"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcGydtPdqswd"
      },
      "source": [
        "This object has fields that determine the structure behind the synthesized data. For example, the sequence lengths, where the active site starts in a sequence, how long the active site is, the class proportion, the intensity of the positive class signal (as described in my report), emission probability distributions, transition mutation probabilities, and some others."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AUhpXaAqcPr",
        "outputId": "0ba1ad44-4fc9-4c3d-d73e-d80977a2f9dd"
      },
      "source": [
        "print(gen.__dict__.keys())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['seq_length', 'start', 'active_site_length', 'p', 'class_signal', 'aa_list', 'background_emission', 'state0_emission', 'state1_emissions', 'transmat', 'startprob', 'emissionprob', 'n_components', 'model'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A9DhrzDrTJj"
      },
      "source": [
        "The `HMMGenerator` class is a wrapper around a Multinomial Hidden Markov Model implemented in `hmmlearn`. The field `gen.model` contains the `hmmlearn` model used to synthesize data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzDScitLrY18",
        "outputId": "229216ca-6b69-4f36-95a4-690a93e9830f"
      },
      "source": [
        "type(gen.model)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "hmmlearn.hmm.MultinomialHMM"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f--TK3jGr2cU"
      },
      "source": [
        "## Using the generator to classify novel sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSS-zcc-rs15"
      },
      "source": [
        "The `HMMGenerator` class is capable of predicting the class 1 probability of a sequence under the existing HMM. Suppose we take a test item."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLniEQTCsNxf"
      },
      "source": [
        "x = X_test[42]"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RQ5JSTZsQ6A",
        "outputId": "bef2eb32-01f4-4db1-ccc3-36dc2238d470"
      },
      "source": [
        "y = y_test[42]\n",
        "y"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3Bgp-n1sV-O"
      },
      "source": [
        "This is a negative instance. Make a mutation at position 35."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc4zI0idsTnp",
        "outputId": "e595bbb1-19ab-4c8d-a290-8242b40b2fc4"
      },
      "source": [
        "x"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08kDe3T4scrn",
        "outputId": "90b90fb3-425f-4323-cd38-1c715a3e3680"
      },
      "source": [
        "x[25]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpRgyOKushHR",
        "outputId": "ba370c09-015a-43eb-8014-9133e512c91e"
      },
      "source": [
        "old_char_idx = np.argmax(x[25])\n",
        "x[25][old_char_idx]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WgZCW7tsmGp"
      },
      "source": [
        "x[25][old_char_idx] = 0.0"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXWnOW0Kso6i"
      },
      "source": [
        "x[25][11] = 1.0"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FIPVBg1GsyUx",
        "outputId": "363fccad-76ff-4979-de57-c0330e11e359"
      },
      "source": [
        "aa_vocab[16]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'T'"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "g1JtGQUYtRiu",
        "outputId": "50fdca30-ca35-4999-88d8-5ba7102447f1"
      },
      "source": [
        "aa_vocab[11]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'K'"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUf7w8xKssUD"
      },
      "source": [
        "This corresponds to substituting the 16th character in the alphabet ('T') with the 11th character ('K'). Now we can predict the class label using the model (after reshaping into a batch of size 1), and predict the class label under the generator model (after converted to a sequence of indices rather than one-hot encoding)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6LT7BzqsrgZ",
        "outputId": "f384d677-c82c-4d1e-c9cb-2e26a50a8a79"
      },
      "source": [
        "# Model prediction on mutation\n",
        "model.predict(x.reshape(1,60,20))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9999988]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOZ99415vFNy",
        "outputId": "82da49b2-4ce5-44c2-87e1-64bcc52a35f3"
      },
      "source": [
        "# Generator posterior prob of positive class\n",
        "gen.predict_proba(np.argmax(x, axis=1))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999979333135554"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soXW1gZLvTOR"
      },
      "source": [
        "Make a few more mutations that I know should make it look like a positive sequence, then see that the generator indicates this looks more like a positive sequence, and the model correctly predicts this as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyLbtxmZvfha"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set the following characters starting at index 26: 'RSFIED'\n",
        "chars = 'RSFIED'\n",
        "\n",
        "for i in range(26, 32):\n",
        "    # Get the new char index\n",
        "    char = chars[i-26]\n",
        "    new_char_idx = aa_vocab.index(char)\n",
        "\n",
        "    # Reset current char to 0\n",
        "    x[i][np.argmax(x[i])] = 0.0\n",
        "\n",
        "    # Make substitution for new char\n",
        "    x[i][new_char_idx] = 1.0"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmqNxzslwjt4",
        "outputId": "f161f3bd-dd3d-45c0-d88b-6b64ab5124b1"
      },
      "source": [
        "# Generator posterior prob of positive class\n",
        "gen.predict_proba(np.argmax(x, axis=1))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999999978548146"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSwtJYR1wg-g",
        "outputId": "2471d342-771f-4ce4-af6a-83194964ce81"
      },
      "source": [
        "# Model prediction on mutation\n",
        "model.predict(x.reshape(1,60,20))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}