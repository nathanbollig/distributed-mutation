{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "explore_data.ipynb",
      "provenance": [],
      "mount_file_id": "https://github.com/nathanbollig/distributed-mutation/blob/main/explore_data.ipynb",
      "authorship_tag": "ABX9TyPCLKL0+g8iS8E8XE0Bf4lA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nathanbollig/distributed-mutation/blob/main/explore_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY1Lfh5kgtL7"
      },
      "source": [
        "# Explore Synthetic Data and Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKPEjUI1g0RJ"
      },
      "source": [
        "In Phase 1 and 2 of this project, synthetic viral sequence data is generated and a classification model is trained on 80% of this data. The result of running the `phases_1_and_2.py` script is a set results objects saved to disk into 6 pickle files. After performing Phase 1 and Phase 2, this notebook loads in these files and explores their contents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddXSXZAsxPkP"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "import pickle"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO0lhqPxgnrO"
      },
      "source": [
        "## Phase 1 and 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZauKNjQ9gum7"
      },
      "source": [
        "This section follows the instructions in the project README to generate data and train a classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hne7sMQngizy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee9f64c5-8940-416d-a65a-50b57406ea96"
      },
      "source": [
        "# Clone the git repository\n",
        "!git clone https://github.com/nathanbollig/distributed-mutation"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'distributed-mutation'...\n",
            "remote: Enumerating objects: 52, done.\u001b[K\n",
            "remote: Counting objects: 100% (52/52), done.\u001b[K\n",
            "remote: Compressing objects: 100% (51/51), done.\u001b[K\n",
            "remote: Total 52 (delta 24), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (52/52), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FFupudVUg86t",
        "outputId": "0da22543-acd7-4ef7-b8a3-b66fdacf03c1"
      },
      "source": [
        "# Apply requirements.txt\n",
        "%cd distributed-mutation\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/distributed-mutation\n",
            "Collecting hmmlearn==0.2.6\n",
            "  Downloading hmmlearn-0.2.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting Keras==2.2.4\n",
            "  Downloading Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
            "\u001b[K     |████████████████████████████████| 312 kB 45.0 MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.22.2\n",
            "  Downloading scikit_learn-0.22.2-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1 MB 17.1 MB/s \n",
            "\u001b[?25hCollecting tensorflow==1.15.5\n",
            "  Downloading tensorflow-1.15.5-cp37-cp37m-manylinux2010_x86_64.whl (110.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5 MB 1.3 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.7/dist-packages (from hmmlearn==0.2.6->-r requirements.txt (line 1)) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.7/dist-packages (from hmmlearn==0.2.6->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.4->-r requirements.txt (line 2)) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.4->-r requirements.txt (line 2)) (1.1.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.4->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from Keras==2.2.4->-r requirements.txt (line 2)) (1.15.0)\n",
            "Collecting keras-applications>=1.0.6\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.22.2->-r requirements.txt (line 3)) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->-r requirements.txt (line 4)) (3.3.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->-r requirements.txt (line 4)) (0.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->-r requirements.txt (line 4)) (1.1.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 42.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->-r requirements.txt (line 4)) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->-r requirements.txt (line 4)) (1.42.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->-r requirements.txt (line 4)) (3.17.3)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 56.0 MB/s \n",
            "\u001b[?25hCollecting numpy>=1.10\n",
            "  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 39.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->-r requirements.txt (line 4)) (1.13.3)\n",
            "Collecting h5py\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 33.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->-r requirements.txt (line 4)) (0.37.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.5->-r requirements.txt (line 4)) (0.12.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->-r requirements.txt (line 4)) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->-r requirements.txt (line 4)) (3.3.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->-r requirements.txt (line 4)) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->-r requirements.txt (line 4)) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->-r requirements.txt (line 4)) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.5->-r requirements.txt (line 4)) (3.6.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7554 sha256=6340610b8fa6a2554cf0cb4dcf6ce67d315ac38f8d630a96e1b0c71f41a0c5ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: numpy, h5py, tensorflow-estimator, tensorboard, scikit-learn, keras-applications, gast, tensorflow, Keras, hmmlearn\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.7.0\n",
            "    Uninstalling tensorboard-2.7.0:\n",
            "      Successfully uninstalled tensorboard-2.7.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.1\n",
            "    Uninstalling scikit-learn-1.0.1:\n",
            "      Successfully uninstalled scikit-learn-1.0.1\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "  Attempting uninstall: Keras\n",
            "    Found existing installation: keras 2.7.0\n",
            "    Uninstalling keras-2.7.0:\n",
            "      Successfully uninstalled keras-2.7.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.6 requires tensorflow>=2.0.0, but you have tensorflow 1.15.5 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.22.2 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed Keras-2.2.4 gast-0.2.2 h5py-2.10.0 hmmlearn-0.2.6 keras-applications-1.0.8 numpy-1.18.5 scikit-learn-0.22.2 tensorboard-1.15.0 tensorflow-1.15.5 tensorflow-estimator-1.15.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "h5py",
                  "keras",
                  "numpy",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtauIY2Xi6Mr"
      },
      "source": [
        "The above conflicts are acceptable because these conflicts are with pre-installed packages in this environment that I don't believe are needed for executing my code. May now need to restart the runtime (and therefore change directory again)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhfwCQ9BiifG",
        "outputId": "0d750eea-d0fd-4145-fd43-fa571de1d892"
      },
      "source": [
        "# Run script\n",
        "%cd distributed-mutation\n",
        "!bash phases_1_and_2.sh"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/distributed-mutation\n",
            "Using TensorFlow backend.\n",
            "Running data generation and model training...\n",
            "tcmalloc: large alloc 4800004096 bytes == 0x55e94f9c2000 @  0x7f27fb31a001 0x7f27f8ebe7b5 0x7f27f8f22c00 0x7f27f8f24a9f 0x7f27f8fbb078 0x55e92ca6f544 0x55e92ca6f240 0x55e92cae3627 0x55e92cadd9ee 0x55e92ca70bda 0x55e92cade915 0x55e92ca70afa 0x55e92cade915 0x55e92cadd9ee 0x55e92ca70bda 0x55e92cadf737 0x55e92cadd9ee 0x55e92cadd6f3 0x55e92cba74c2 0x55e92cba783d 0x55e92cba76e6 0x55e92cb7f163 0x55e92cb7ee0c 0x7f27fa102bf7 0x55e92cb7ecea\n",
            "tcmalloc: large alloc 3840000000 bytes == 0x55ea6db66000 @  0x7f27fb3181e7 0x7f27f8ebe631 0x7f27f8f22cc8 0x7f27f8f22de3 0x7f27f8fadf06 0x7f27f8fae368 0x55e92cb57409 0x55e92cadee7a 0x55e92cadd9ee 0x55e92ca70bda 0x55e92cadf737 0x55e92cadd9ee 0x55e92ca70bda 0x55e92cade915 0x55e92cb61cf8 0x55e92cb57cae 0x55e92cb47ae5 0x55e92ca7e224 0x55e92caaf0a4 0x55e92ca6fc52 0x55e92cae2c25 0x55e92caddced 0x55e92ca70bda 0x55e92cadf737 0x55e92cadd9ee 0x55e92ca70bda 0x55e92cadf737 0x55e92cadd9ee 0x55e92cadd6f3 0x55e92cba74c2 0x55e92cba783d\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "2021-11-26 22:21:37.417685: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2021-11-26 22:21:37.423478: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2199995000 Hz\n",
            "2021-11-26 22:21:37.423744: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e92e01aa00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2021-11-26 22:21:37.423785: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "800000/800000 [==============================] - 19s 24us/step - loss: 0.0919 - acc: 0.9740\n",
            "Epoch 2/10\n",
            "800000/800000 [==============================] - 18s 22us/step - loss: 0.0562 - acc: 0.9792\n",
            "Epoch 3/10\n",
            "800000/800000 [==============================] - 18s 22us/step - loss: 0.0561 - acc: 0.9792\n",
            "Epoch 4/10\n",
            "800000/800000 [==============================] - 18s 22us/step - loss: 0.0561 - acc: 0.9792\n",
            "Epoch 5/10\n",
            "800000/800000 [==============================] - 18s 22us/step - loss: 0.0561 - acc: 0.9793\n",
            "Epoch 6/10\n",
            "800000/800000 [==============================] - 18s 22us/step - loss: 0.0561 - acc: 0.9793\n",
            "Epoch 7/10\n",
            "800000/800000 [==============================] - 18s 22us/step - loss: 0.0561 - acc: 0.9792\n",
            "Epoch 8/10\n",
            "800000/800000 [==============================] - 17s 22us/step - loss: 0.0561 - acc: 0.9793\n",
            "Epoch 9/10\n",
            "800000/800000 [==============================] - 18s 22us/step - loss: 0.0561 - acc: 0.9793\n",
            "Epoch 10/10\n",
            "800000/800000 [==============================] - 18s 22us/step - loss: 0.0561 - acc: 0.9792\n",
            "Train Accuracy: 97.97\n",
            "Validation Accuracy: 97.90\n",
            "Printing test dataset to text file...\n",
            "tcmalloc: large alloc 3840008192 bytes == 0x55e933e8c000 @  0x7f27fb3181e7 0x55e92caa0f98 0x55e92ca6be27 0x7f27f8fb1f0d 0x55e92cb8490c 0x55e92ca6f6f2 0x7f27f8fb2bb8 0x55e92cb8490c 0x55e92ca703ce 0x55e92ca702f9 0x55e92cb20f94 0x55e92cb208a8 0x55e92cb1ff3c 0x55e92ca11992 0x55e92ca6f46c 0x55e92ca6f240 0x55e92cae3627 0x55e92cadd9ee 0x55e92cadd6f3 0x55e92cba74c2 0x55e92cba783d 0x55e92cba76e6 0x55e92cb7f163 0x55e92cb7ee0c 0x7f27fa102bf7 0x55e92cb7ecea\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3KxablrkQMw"
      },
      "source": [
        "## Explore Phase 1 and 2 Output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qg26JS-TkcI3"
      },
      "source": [
        "### Read in results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhXcQyLpkf1E",
        "outputId": "b78974d1-8cf5-4e67-a355-f3a9ad2ec095"
      },
      "source": [
        "# Load model\n",
        "from tensorflow import keras\n",
        "model = keras.models.load_model('model.tf')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_daNhizw-1S5"
      },
      "source": [
        "# Load pickled objects\n",
        "with open(\"aa_vocab.pkl\", 'rb') as pfile:\n",
        "    aa_vocab = pickle.load(pfile)\n",
        "with open(\"generator.pkl\", 'rb') as pfile:\n",
        "    gen = pickle.load(pfile)\n",
        "with open(\"result.pkl\", 'rb') as pfile:\n",
        "    result = pickle.load(pfile)\n",
        "\n",
        "# Load data objects\n",
        "with open(\"data_train.pkl\", 'rb') as pfile:\n",
        "    data_train = pickle.load(pfile)\n",
        "with open(\"data_val.pkl\", 'rb') as pfile:\n",
        "    data_val = pickle.load(pfile)\n",
        "with open(\"data_test.pkl\", 'rb') as pfile:\n",
        "    data_test = pickle.load(pfile)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZo2LYeUalQt"
      },
      "source": [
        "# Unpack data objects\n",
        "X_train, y_train = data_train\n",
        "X_val, y_val = data_val\n",
        "X_test, y_test = data_test"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oyr3KMlOkHTr"
      },
      "source": [
        "def get_test_data():\n",
        "    # Load test data\n",
        "    with open('data_test.txt') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    '''\n",
        "    Convert string representation of sequence in `sequences` to integer-encoded list\n",
        "    '''\n",
        "    def seq_str_to_list(s):\n",
        "        seq = s.split(',')\n",
        "        return list(map(int, seq))\n",
        "\n",
        "    # Read file into lists\n",
        "    X = []\n",
        "    y = []\n",
        "    for line in lines:\n",
        "        split_line = line.split(',')\n",
        "        seq = split_line[0:-1]\n",
        "        label = int(split_line[-1])\n",
        "        X.append(seq)\n",
        "        y.append(label)\n",
        "\n",
        "    # Convert lists to arrays\n",
        "    X = np.array(X, dtype=int)\n",
        "    y = np.array(y, dtype=int)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "X, y = get_test_data()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33On35Stll3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06315dad-cf01-4142-88f4-aa054c9480e0"
      },
      "source": [
        "X[5]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1,  6, 17,  1, 17,  1, 10,  2, 14,  8, 10, 19,  0, 11, 14, 10, 10,\n",
              "        0, 10,  0, 11, 17, 15,  9,  0, 11, 16, 15, 13, 16,  5, 18, 10, 10,\n",
              "       11, 19, 11,  5,  8,  2, 19,  5, 13, 15,  9, 10,  8, 13, 10, 14,  0,\n",
              "        1,  3, 13,  2,  5, 19,  8, 16,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4TeNcJdloW_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3111947e-a192-4aeb-e74e-a07ca5f588b9"
      },
      "source": [
        "y[5]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifFVCGFClpRu"
      },
      "source": [
        "### Model and validation results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYlC3qNbluus"
      },
      "source": [
        "The Keras model object is stored in `model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVZIgrkcluEk",
        "outputId": "25a49dd2-d864-48da-90ee-67e3496f0fbf"
      },
      "source": [
        "type(model)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.keras.engine.sequential.Sequential"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sANnES3Rl6ki"
      },
      "source": [
        "During the training of this model, a dictionary was created of training set and validation set performance. We can display the values in this dictionary to see the accuracy of the stored model on its training and validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhYGeA76mBRb",
        "outputId": "3d9eb250-b152-442c-a618-0d823d166542"
      },
      "source": [
        "result"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'model_train_accuracy': 0.979705, 'model_val_accuracy': 0.97897}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm3-FWr5mQwg"
      },
      "source": [
        "### Synthetic sequence data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbJubNbQmTxG"
      },
      "source": [
        "The data are sequences (variable names prefixed with `X`) and their labels (prefixed with `y`). Data is split into training, validation, and test sets. The training set was used to train the model and the validation set was used to measure the performance reported in the `result` dictionary. The test set has not yet been used for model training or evaluation.\n",
        "\n",
        "The data should be 80% training, 10% validation, and 10% test, with the total number of sequences as specified in the `phases_1_and_2.sh` script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFL6G855m_g8",
        "outputId": "ec300903-d87f-4cec-fad8-06a91a3ca22c"
      },
      "source": [
        "len(X_train), len(X_val), len(X_test)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800000, 100000, 100000)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrbJmM0YnJjn"
      },
      "source": [
        "As expected, there are the same number of labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13crO8qCnHj1",
        "outputId": "b22d8d8a-f3bb-4221-cfd0-458b20b65154"
      },
      "source": [
        "len(y_train), len(y_val), len(y_test)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800000, 100000, 100000)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jz3PLhOPnQk7"
      },
      "source": [
        "The sequence variables (`X_`*) are numpy arrays, where each sequence is represented by a 60 x 20 matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm53fyLVnSV3",
        "outputId": "80dbb1b3-d95d-4b6e-9766-b6701aeba494"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(800000, 60, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGrWHlrVnlOB"
      },
      "source": [
        "Each of the 60 positions in the sequence is represented by a one-hot vector of length 20. We assume that 20 is the size of the character alphabet. For example, a single sequence looks like the following matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-iN9FUNnmPA",
        "outputId": "fa1848f1-ab77-4b57-8614-8e95bb2f5e69"
      },
      "source": [
        "X_train[42]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6qHBcU8omr5"
      },
      "source": [
        "The binary sequence label is an integer. The value 1 represents positive, and 0 represents negative. For example, the label for the above sequence is shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPnpjB-CoxOI",
        "outputId": "77b1bf6e-2547-4f31-c4dd-ac4f4f1a91da"
      },
      "source": [
        "y_train[42]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O67nQig0oiBE"
      },
      "source": [
        "### Applying the model to a sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9nQXZ-ooz3H"
      },
      "source": [
        "We can apply the stored model to a sequence to get a prediction, using the TensorFlow model objects's API. For example, suppose we want to apply the model to sequences at index 42 and 43 in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMrF2REeo8NN",
        "outputId": "58fe6067-fe25-4040-9d2a-a9ce9c869818"
      },
      "source": [
        "model.predict(X_train[42:44])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00095886],\n",
              "       [0.01204966]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZumGgHzLpXjN"
      },
      "source": [
        "We see the prediction for each sequence as a number between 0 and 1. In this case, they are both close to 1, indicating higher confidence of positive. These predictions are correct in this context, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kik3JggMplnq",
        "outputId": "c0acbd76-2961-4e05-ff80-0969de83bd7c"
      },
      "source": [
        "y_train[42:44]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VNE61vkppoV"
      },
      "source": [
        "### Amino acid vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uQ7V2OijpvPI"
      },
      "source": [
        "The pickle file also included the amino acid vocabulary used for the encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3gHAGRHp3V1",
        "outputId": "49ca2d15-2334-4f40-df04-10e954a93a17"
      },
      "source": [
        "print(aa_vocab)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drf89Wh7p8tb",
        "outputId": "f7e85dd0-c493-4376-fc4c-1b3239af1630"
      },
      "source": [
        "len(aa_vocab)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIOgt0aSp61b"
      },
      "source": [
        "This associates an index in the range 0-19 (as described above in relation to the one-hot representation of sequences) to a specific character that reflects an amino acid in the biological sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "974voIvGqOzR"
      },
      "source": [
        "### Markov model (HMMGenerator object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbOH7Md5qTx_"
      },
      "source": [
        "Finally, the pickle file includes the `HMMGenerator` object used to synthesize the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4tN92tYqZMS",
        "outputId": "9b16f505-8fba-401e-c656-dadcb295bcaa"
      },
      "source": [
        "type(gen)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HMM_generator_motif.HMMGenerator"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcGydtPdqswd"
      },
      "source": [
        "This object has fields that determine the structure behind the synthesized data. For example, the sequence lengths, where the active site starts in a sequence, how long the active site is, the class proportion, the intensity of the positive class signal (as described in my report), emission probability distributions, transition mutation probabilities, and some others."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AUhpXaAqcPr",
        "outputId": "be00413c-1333-47ab-ab0f-7750d524209a"
      },
      "source": [
        "print(gen.__dict__.keys())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['seq_length', 'start', 'active_site_length', 'p', 'class_signal', 'aa_list', 'background_emission', 'state0_emission', 'state1_emissions', 'transmat', 'startprob', 'emissionprob', 'n_components', 'model'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A9DhrzDrTJj"
      },
      "source": [
        "The `HMMGenerator` class is a wrapper around a Multinomial Hidden Markov Model implemented in `hmmlearn`. The field `gen.model` contains the `hmmlearn` model used to synthesize data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzDScitLrY18",
        "outputId": "7b08203b-b16c-44b4-f9d7-ece92a6faafb"
      },
      "source": [
        "type(gen.model)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "hmmlearn.hmm.MultinomialHMM"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f--TK3jGr2cU"
      },
      "source": [
        "## Using the generator to classify novel sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSS-zcc-rs15"
      },
      "source": [
        "The `HMMGenerator` class is capable of predicting the class 1 probability of a sequence under the existing HMM. Suppose we take a test item."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLniEQTCsNxf"
      },
      "source": [
        "x = X_test[52]"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RQ5JSTZsQ6A",
        "outputId": "5b3cecf0-857c-4db0-af04-7e51dedf6b7b"
      },
      "source": [
        "y = y_test[52]\n",
        "y"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3Bgp-n1sV-O"
      },
      "source": [
        "This is a negative instance. Make a mutation at position 25."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qc4zI0idsTnp",
        "outputId": "c0bd0da3-7bd1-4da4-cf0f-1df18f33a4bc"
      },
      "source": [
        "x"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08kDe3T4scrn",
        "outputId": "5d301e3e-30a0-4b4d-e3f9-376c543b8766"
      },
      "source": [
        "x[25]"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0.], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpRgyOKushHR",
        "outputId": "f6421187-1c2f-4fc1-98bd-4d1bf6d0b5dd"
      },
      "source": [
        "old_char_idx = np.argmax(x[25])\n",
        "x[25][old_char_idx]"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WgZCW7tsmGp"
      },
      "source": [
        "x[25][old_char_idx] = 0.0"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXWnOW0Kso6i"
      },
      "source": [
        "x[25][11] = 1.0"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FIPVBg1GsyUx",
        "outputId": "6409b81e-3c14-4f7f-fd02-5fc517afd6e1"
      },
      "source": [
        "aa_vocab[16]"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'T'"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "g1JtGQUYtRiu",
        "outputId": "fdfa765c-79ad-4d12-ce79-7a2cf8d4b97a"
      },
      "source": [
        "aa_vocab[11]"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'K'"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUf7w8xKssUD"
      },
      "source": [
        "This corresponds to substituting the 16th character in the alphabet ('T') with the 11th character ('K'). Now we can predict the class label using the model (after reshaping into a batch of size 1), and predict the class label under the generator model (after converted to a sequence of indices rather than one-hot encoding)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6LT7BzqsrgZ",
        "outputId": "4a08351c-9c45-44ea-dc4b-3ea2a3b969a4"
      },
      "source": [
        "# Model prediction on mutation\n",
        "model.predict(x.reshape(1,60,20))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02638009]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOZ99415vFNy",
        "outputId": "9b0b62ac-e258-4f1d-fdf7-55f9102fdfa4"
      },
      "source": [
        "# Generator posterior prob of positive class\n",
        "gen.predict_proba(np.argmax(x, axis=1))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.04532741398446242"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soXW1gZLvTOR"
      },
      "source": [
        "Make a few more mutations that I know should make it look like a positive sequence, then see that the generator indicates this looks more like a positive sequence, and the model correctly predicts this as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GyLbtxmZvfha"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set the following characters starting at index 26: 'RSFIED'\n",
        "chars = 'RSFIED'\n",
        "\n",
        "for i in range(26, 32):\n",
        "    # Get the new char index\n",
        "    char = chars[i-26]\n",
        "    new_char_idx = aa_vocab.index(char)\n",
        "\n",
        "    # Reset current char to 0\n",
        "    x[i][np.argmax(x[i])] = 0.0\n",
        "\n",
        "    # Make substitution for new char\n",
        "    x[i][new_char_idx] = 1.0"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmqNxzslwjt4",
        "outputId": "59cafaa5-0ea6-4801-a4a9-f86592dc277d"
      },
      "source": [
        "# Generator posterior prob of positive class\n",
        "gen.predict_proba(np.argmax(x, axis=1))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9999998716679468"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSwtJYR1wg-g",
        "outputId": "95e6d235-bc9c-4755-cead-4b3961f22abc"
      },
      "source": [
        "# Model prediction on mutation\n",
        "model.predict(x.reshape(1,60,20))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9999999]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6jl9msjGBFs"
      },
      "source": [
        "### Copy pickle files to Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtLxgnQBG1O4"
      },
      "source": [
        "First need to mount Google Drive for this to work, and have a folder there called '744'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az_9ejuMGELU"
      },
      "source": [
        "# Copy files to Drive\n",
        "\n",
        "!cp *.* ../drive/MyDrive/744/"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDsjTV3qh7qz"
      },
      "source": [
        "Create shortened version of test data and push to Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uxMo0uffZL8"
      },
      "source": [
        "lines = []\n",
        "N = 100\n",
        "count = 0\n",
        "with open('data_test.txt') as f_in:\n",
        "    for line in f_in:\n",
        "        if count > N:\n",
        "            break\n",
        "        else:\n",
        "            lines.append(line)\n",
        "            count += 1\n",
        "\n",
        "with open('data_test_short.txt', 'w') as f:\n",
        "    for line in lines:\n",
        "        f.write(line)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0wYqB59grE8"
      },
      "source": [
        "!cp data_test_short.txt ../drive/MyDrive/744/"
      ],
      "execution_count": 67,
      "outputs": []
    }
  ]
}