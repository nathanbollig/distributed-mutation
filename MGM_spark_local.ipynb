{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MGM_spark_local.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "13UE2tnmPkgI_4dQbAQGV6OhvMR70zzmS",
      "authorship_tag": "ABX9TyOMUk6TvCfF+iev1mPE3As6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nathanbollig/distributed-mutation/blob/main/MGM_spark_local.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzAqnG_DdT2E"
      },
      "source": [
        "# Copy files from cloud storage - need to connect Google Drive\n",
        "!cp drive/MyDrive/744/*.pkl .\n",
        "!cp drive/MyDrive/744/*.txt .\n",
        "!cp drive/MyDrive/744/*.tf .\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6nSgFcIdeQ7",
        "outputId": "760da9c7-80b5-409e-c970-6655ad965a62"
      },
      "source": [
        "# Install pyspark\n",
        "!pip install pyspark\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.3 MB 38 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.2\n",
            "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 65.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=a8cf5898c3e8607d1b246255aa64379dc56470bbca1e8775ac07d43c3d5b1d7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/de/d2/9be5d59d7331c6c2a7c1b6d1a4f463ce107332b1ecd4e80718\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EI_AJpXg01N",
        "outputId": "f6953e27-093e-481e-b838-c946c5531fa5"
      },
      "source": [
        "# Run the application\n",
        "! python mgm_local.py --conf_thresh 0.9 --data_path 'data_test_short.txt'\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: An illegal reflective access operation has occurred\n",
            "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/lib/python3.7/dist-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
            "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
            "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
            "WARNING: All illegal access operations will be denied in a future release\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "21/11/26 23:12:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "21/11/26 23:12:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
            "+--------------------+---+\n",
            "|                 seq| id|\n",
            "+--------------------+---+\n",
            "|8,4,0,7,7,0,0,0,1...|  0|\n",
            "|15,2,7,14,14,10,2...|  1|\n",
            "|17,10,8,3,4,7,2,1...|  2|\n",
            "|3,6,16,16,3,10,12...|  3|\n",
            "|16,0,7,0,15,16,14...|  4|\n",
            "|18,1,5,19,14,19,1...|  5|\n",
            "|1,10,12,11,11,0,1...|  6|\n",
            "|7,3,0,9,7,2,0,19,...|  7|\n",
            "|19,16,14,6,5,3,4,...|  8|\n",
            "|5,11,15,0,19,1,18...|  9|\n",
            "|12,10,15,16,11,5,...| 10|\n",
            "|10,5,17,15,8,14,1...| 11|\n",
            "|15,6,2,8,11,10,0,...| 12|\n",
            "|9,16,11,13,14,19,...| 13|\n",
            "|15,19,7,12,14,5,3...| 14|\n",
            "|10,9,19,16,14,6,1...| 15|\n",
            "|15,2,10,13,15,7,2...| 16|\n",
            "|9,8,1,14,8,16,11,...| 17|\n",
            "|14,11,15,8,2,14,1...| 18|\n",
            "|11,0,14,10,17,10,...| 19|\n",
            "+--------------------+---+\n",
            "only showing top 20 rows\n",
            "\n",
            "2021-11-26 23:12:51.339437: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-11-26 23:12:52.333212: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "2021-11-26 23:13:04.312170: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-11-26 23:13:04.762309: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-11-26 23:13:16.668757: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-11-26 23:13:34.912056: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-11-26 23:13:42.638774: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-11-26 23:14:00.773645: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2021-11-26 23:14:10.803848: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVolh6jHeUfn"
      },
      "source": [
        "# Copy files to Google Drive\n",
        "!cp *.csv drive/MyDrive/744/\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}